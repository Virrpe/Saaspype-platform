#!/usr/bin/env python3
"""
Industry Standard Scoring System
Upgrade your trend analysis to match Bloomberg/CB Insights standards
"""

import numpy as np
import statistics
import math
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
from dataclasses import dataclass

@dataclass
class IndustryMetrics:
    """Industry-standard metrics"""
    momentum_score: float
    confidence_score: float
    statistical_significance: float
    trend_velocity: float
    market_penetration: float
    competitive_intensity: float
    innovation_index: float
    risk_score: float
    opportunity_score: float
    
    # Advanced metrics
    signal_quality_score: float
    source_diversity_score: float
    temporal_consistency_score: float
    cross_validation_score: float

class IndustryStandardScorer:
    """Industry-grade scoring system"""
    
    def __init__(self):
        # Industry-calibrated weights (based on CB Insights, Bloomberg methodology)
        self.scoring_weights = {
            'momentum': 0.25,
            'velocity': 0.20,
            'penetration': 0.15,
            'innovation': 0.15,
            'confidence': 0.10,
            'quality': 0.10,
            'risk_adjustment': 0.05
        }
        
        # Statistical thresholds (industry standard)
        self.thresholds = {
            'min_signals_for_trend': 15,
            'min_sources_for_validation': 3,
            'significance_level': 0.05,
            'confidence_threshold': 0.80,
            'high_engagement_threshold': 50,
            'velocity_window_hours': 72
        }
        
        # Source authority scores (industry calibrated)
        self.source_authority = {
            'reddit': 0.70,
            'github': 0.85,
            'stackoverflow': 0.80,
            'hacker_news': 0.90,
            'dev_to': 0.75,
            'twitter': 0.65,
            'linkedin': 0.85,
            'techcrunch': 0.95,
            'crunchbase': 0.98
        }
        
        # Market size indicators (TAM estimation)
        self.market_indicators = {
            'enterprise': 2.5,
            'b2b': 2.0,
            'saas': 1.8,
            'platform': 1.6,
            'consumer': 1.2,
            'niche': 0.8
        }
    
    def calculate_industry_score(self, signals: List, keyword: str) -> IndustryMetrics:
        """Calculate industry-standard opportunity score"""
        
        if len(signals) < self.thresholds['min_signals_for_trend']:
            return self._create_low_confidence_metrics()
        
        # 1. Advanced Momentum Calculation
        momentum_score = self._calculate_advanced_momentum(signals)
        
        # 2. Trend Velocity Analysis
        velocity_score = self._calculate_trend_velocity(signals)
        
        # 3. Market Penetration Assessment
        penetration_score = self._calculate_market_penetration(signals)
        
        # 4. Innovation Index
        innovation_score = self._calculate_innovation_index(signals, keyword)
        
        # 5. Statistical Confidence
        confidence_score, significance = self._calculate_statistical_confidence(signals)
        
        # 6. Signal Quality Assessment
        quality_score = self._calculate_signal_quality(signals)
        
        # 7. Risk Assessment
        risk_score = self._calculate_comprehensive_risk(signals)
        
        # 8. Cross-validation scores
        diversity_score = self._calculate_source_diversity(signals)
        temporal_score = self._calculate_temporal_consistency(signals)
        cross_validation_score = self._calculate_cross_validation(signals)
        
        # 9. Competitive Intensity
        competitive_intensity = self._calculate_competitive_intensity(signals)
        
        # 10. Final Opportunity Score (weighted combination)
        opportunity_score = self._calculate_weighted_opportunity_score({
            'momentum': momentum_score,
            'velocity': velocity_score,
            'penetration': penetration_score,
            'innovation': innovation_score,
            'confidence': confidence_score,
            'quality': quality_score,
            'risk': risk_score
        })
        
        return IndustryMetrics(
            momentum_score=momentum_score,
            confidence_score=confidence_score,
            statistical_significance=significance,
            trend_velocity=velocity_score,
            market_penetration=penetration_score,
            competitive_intensity=competitive_intensity,
            innovation_index=innovation_score,
            risk_score=risk_score,
            opportunity_score=opportunity_score,
            signal_quality_score=quality_score,
            source_diversity_score=diversity_score,
            temporal_consistency_score=temporal_score,
            cross_validation_score=cross_validation_score
        )
    
    def _calculate_advanced_momentum(self, signals: List) -> float:
        """Industry-standard momentum calculation with exponential decay"""
        if not signals:
            return 0.0
        
        current_time = datetime.now()
        total_weighted_score = 0
        total_weight = 0
        
        for signal in signals:
            # Time decay (exponential with 72-hour half-life)
            hours_ago = (current_time - signal.timestamp).total_seconds() / 3600
            time_weight = math.exp(-hours_ago / 72)  # 72-hour decay
            
            # Source authority weight
            authority_weight = self.source_authority.get(signal.source, 0.5)
            
            # Engagement normalization (log scale for viral content)
            normalized_engagement = math.log(1 + signal.engagement_score)
            
            # Sentiment boost
            sentiment_multiplier = 1 + (signal.sentiment_score - 0.5) * 0.3
            
            # Combined weight
            combined_weight = time_weight * authority_weight * sentiment_multiplier
            
            total_weighted_score += normalized_engagement * combined_weight
            total_weight += combined_weight
        
        raw_momentum = total_weighted_score / total_weight if total_weight > 0 else 0
        
        # Scale to 0-10 with industry calibration
        return min(10.0, raw_momentum * 1.5)
    
    def _calculate_trend_velocity(self, signals: List) -> float:
        """Calculate trend acceleration/deceleration"""
        if len(signals) < 3:
            return 0.0
        
        # Sort signals by time
        sorted_signals = sorted(signals, key=lambda x: x.timestamp)
        
        # Calculate engagement velocity over time windows
        window_hours = 24
        velocities = []
        
        for i in range(len(sorted_signals) - 1):
            time_diff = (sorted_signals[i+1].timestamp - sorted_signals[i].timestamp).total_seconds() / 3600
            if time_diff > 0:
                engagement_diff = sorted_signals[i+1].engagement_score - sorted_signals[i].engagement_score
                velocity = engagement_diff / time_diff
                velocities.append(velocity)
        
        if not velocities:
            return 0.0
        
        # Average velocity with recent bias
        weighted_velocities = []
        for i, velocity in enumerate(velocities):
            weight = (i + 1) / len(velocities)  # Recent signals get higher weight
            weighted_velocities.append(velocity * weight)
        
        avg_velocity = sum(weighted_velocities) / len(weighted_velocities)
        
        # Normalize to 0-10 scale
        return min(10.0, max(0.0, (avg_velocity + 10) / 2))
    
    def _calculate_market_penetration(self, signals: List) -> float:
        """Calculate market penetration across sources and demographics"""
        if not signals:
            return 0.0
        
        # Source diversity
        unique_sources = len(set(signal.source for signal in signals))
        source_penetration = min(1.0, unique_sources / 8)  # 8 major sources
        
        # Engagement distribution
        engagement_scores = [s.engagement_score for s in signals]
        if len(engagement_scores) > 1:
            # High engagement signals indicate broader appeal
            high_engagement_ratio = len([s for s in engagement_scores if s > self.thresholds['high_engagement_threshold']]) / len(engagement_scores)
        else:
            high_engagement_ratio = 0.5
        
        # Temporal spread
        if len(signals) > 1:
            time_span = (max(s.timestamp for s in signals) - min(s.timestamp for s in signals)).total_seconds() / 3600
            temporal_penetration = min(1.0, time_span / 168)  # 1 week = full penetration
        else:
            temporal_penetration = 0.1
        
        # Combined penetration score
        penetration = (source_penetration * 0.4 + high_engagement_ratio * 0.4 + temporal_penetration * 0.2)
        
        return penetration * 10  # Scale to 0-10
    
    def _calculate_innovation_index(self, signals: List, keyword: str) -> float:
        """Calculate innovation/novelty index"""
        if not signals:
            return 0.0
        
        # Keyword novelty (simplified - would use NLP in production)
        emerging_keywords = ['ai', 'web3', 'quantum', 'metaverse', 'defi', 'nft']
        established_keywords = ['email', 'website', 'database', 'server']
        
        novelty_score = 0.5  # Default
        keyword_lower = keyword.lower()
        
        if any(ek in keyword_lower for ek in emerging_keywords):
            novelty_score = 0.8
        elif any(ek in keyword_lower for ek in established_keywords):
            novelty_score = 0.2
        
        # Technology complexity indicators
        tech_indicators = ['api', 'ml', 'ai', 'blockchain', 'automation', 'platform']
        tech_mentions = sum(1 for signal in signals for indicator in tech_indicators 
                           if indicator in signal.content.lower())
        
        tech_complexity = min(1.0, tech_mentions / (len(signals) * 2))
        
        # Market timing indicators
        timing_indicators = ['new', 'emerging', 'trending', 'growing', 'hot']
        timing_mentions = sum(1 for signal in signals for indicator in timing_indicators 
                             if indicator in signal.content.lower())
        
        timing_score = min(1.0, timing_mentions / len(signals))
        
        # Combined innovation index
        innovation = (novelty_score * 0.4 + tech_complexity * 0.3 + timing_score * 0.3)
        
        return innovation * 10  # Scale to 0-10
    
    def _calculate_statistical_confidence(self, signals: List) -> Tuple[float, float]:
        """Calculate statistical confidence and significance"""
        if len(signals) < 3:
            return 0.0, 0.0
        
        engagement_scores = [s.engagement_score for s in signals]
        
        # Basic statistical measures
        mean_engagement = statistics.mean(engagement_scores)
        std_engagement = statistics.stdev(engagement_scores) if len(engagement_scores) > 1 else 0
        
        # Confidence based on sample size and consistency
        sample_size_factor = min(1.0, len(signals) / 50)  # 50 signals = full confidence
        consistency_factor = 1.0 - min(1.0, std_engagement / (mean_engagement + 1))
        
        confidence = (sample_size_factor * 0.6 + consistency_factor * 0.4)
        
        # Statistical significance (simplified t-test)
        if std_engagement > 0 and len(signals) > 2:
            baseline = 10.0  # Industry baseline
            t_stat = abs(mean_engagement - baseline) / (std_engagement / math.sqrt(len(signals)))
            # Simplified p-value approximation
            significance = min(1.0, t_stat / 3.0)
        else:
            significance = 0.0
        
        return confidence * 10, significance
    
    def _calculate_signal_quality(self, signals: List) -> float:
        """Assess overall signal quality"""
        if not signals:
            return 0.0
        
        quality_factors = []
        
        # Content length quality
        avg_content_length = sum(len(s.content) for s in signals) / len(signals)
        length_quality = min(1.0, avg_content_length / 200)  # 200 chars = good quality
        quality_factors.append(length_quality)
        
        # Source authority
        avg_authority = sum(self.source_authority.get(s.source, 0.5) for s in signals) / len(signals)
        quality_factors.append(avg_authority)
        
        # Keyword relevance
        keyword_density = sum(len(s.keywords) for s in signals) / len(signals)
        keyword_quality = min(1.0, keyword_density / 5)  # 5 keywords = good
        quality_factors.append(keyword_quality)
        
        # Engagement quality (not just quantity)
        high_quality_signals = len([s for s in signals if s.engagement_score > 20])
        engagement_quality = high_quality_signals / len(signals)
        quality_factors.append(engagement_quality)
        
        return statistics.mean(quality_factors) * 10
    
    def _calculate_comprehensive_risk(self, signals: List) -> float:
        """Calculate comprehensive risk assessment"""
        if not signals:
            return 1.0
        
        risk_factors = []
        
        # Volatility risk
        engagement_scores = [s.engagement_score for s in signals]
        if len(engagement_scores) > 1:
            volatility = statistics.stdev(engagement_scores) / (statistics.mean(engagement_scores) + 1)
            risk_factors.append(min(1.0, volatility))
        
        # Source concentration risk
        source_counts = {}
        for signal in signals:
            source_counts[signal.source] = source_counts.get(signal.source, 0) + 1
        
        max_source_concentration = max(source_counts.values()) / len(signals)
        concentration_risk = max_source_concentration  # Higher concentration = higher risk
        risk_factors.append(concentration_risk)
        
        # Temporal concentration risk
        if len(signals) > 1:
            time_span = (max(s.timestamp for s in signals) - min(s.timestamp for s in signals)).total_seconds() / 3600
            if time_span < 6:  # All signals in 6 hours = risky
                temporal_risk = 1.0 - (time_span / 6)
            else:
                temporal_risk = 0.1
            risk_factors.append(temporal_risk)
        
        # Sentiment risk (too positive might be hype)
        avg_sentiment = sum(s.sentiment_score for s in signals) / len(signals)
        if avg_sentiment > 0.8:  # Too positive = hype risk
            sentiment_risk = (avg_sentiment - 0.8) * 5
        else:
            sentiment_risk = 0.1
        risk_factors.append(sentiment_risk)
        
        return statistics.mean(risk_factors)
    
    def _calculate_source_diversity(self, signals: List) -> float:
        """Calculate source diversity score"""
        if not signals:
            return 0.0
        
        unique_sources = len(set(s.source for s in signals))
        max_possible_sources = 8  # Major sources
        
        return min(10.0, (unique_sources / max_possible_sources) * 10)
    
    def _calculate_temporal_consistency(self, signals: List) -> float:
        """Calculate temporal consistency"""
        if len(signals) < 3:
            return 0.0
        
        # Check if signals are spread over time vs clustered
        sorted_signals = sorted(signals, key=lambda x: x.timestamp)
        time_gaps = []
        
        for i in range(len(sorted_signals) - 1):
            gap = (sorted_signals[i+1].timestamp - sorted_signals[i].timestamp).total_seconds() / 3600
            time_gaps.append(gap)
        
        if not time_gaps:
            return 0.0
        
        # Consistent gaps = higher score
        avg_gap = statistics.mean(time_gaps)
        gap_std = statistics.stdev(time_gaps) if len(time_gaps) > 1 else 0
        
        consistency = 1.0 - min(1.0, gap_std / (avg_gap + 1))
        
        return consistency * 10
    
    def _calculate_cross_validation(self, signals: List) -> float:
        """Calculate cross-source validation score"""
        if len(signals) < self.thresholds['min_sources_for_validation']:
            return 0.0
        
        # Check if multiple sources report similar trends
        source_groups = {}
        for signal in signals:
            if signal.source not in source_groups:
                source_groups[signal.source] = []
            source_groups[signal.source].append(signal)
        
        if len(source_groups) < 3:
            return 0.0
        
        # Calculate correlation between sources (simplified)
        source_avg_engagement = {}
        for source, source_signals in source_groups.items():
            source_avg_engagement[source] = sum(s.engagement_score for s in source_signals) / len(source_signals)
        
        # If multiple sources show similar engagement levels = good validation
        engagement_values = list(source_avg_engagement.values())
        if len(engagement_values) > 1:
            engagement_std = statistics.stdev(engagement_values)
            engagement_mean = statistics.mean(engagement_values)
            validation_score = 1.0 - min(1.0, engagement_std / (engagement_mean + 1))
        else:
            validation_score = 0.0
        
        return validation_score * 10
    
    def _calculate_competitive_intensity(self, signals: List) -> float:
        """Calculate competitive intensity"""
        if not signals:
            return 0.0
        
        competitive_terms = ['competitor', 'alternative', 'vs', 'comparison', 'market leader', 'competition']
        total_mentions = 0
        
        for signal in signals:
            content_lower = signal.content.lower()
            total_mentions += sum(1 for term in competitive_terms if term in content_lower)
        
        intensity = min(1.0, total_mentions / len(signals) / 3)  # Normalize
        return intensity * 10
    
    def _calculate_weighted_opportunity_score(self, scores: Dict) -> float:
        """Calculate final weighted opportunity score"""
        weighted_score = (
            scores['momentum'] * self.scoring_weights['momentum'] +
            scores['velocity'] * self.scoring_weights['velocity'] +
            scores['penetration'] * self.scoring_weights['penetration'] +
            scores['innovation'] * self.scoring_weights['innovation'] +
            scores['confidence'] * self.scoring_weights['confidence'] +
            scores['quality'] * self.scoring_weights['quality']
        )
        
        # Risk adjustment
        risk_adjustment = 1.0 - (scores['risk'] * self.scoring_weights['risk_adjustment'])
        
        return weighted_score * risk_adjustment
    
    def _create_low_confidence_metrics(self) -> IndustryMetrics:
        """Create low confidence metrics for insufficient data"""
        return IndustryMetrics(
            momentum_score=0.0,
            confidence_score=0.0,
            statistical_significance=0.0,
            trend_velocity=0.0,
            market_penetration=0.0,
            competitive_intensity=0.0,
            innovation_index=0.0,
            risk_score=1.0,
            opportunity_score=0.0,
            signal_quality_score=0.0,
            source_diversity_score=0.0,
            temporal_consistency_score=0.0,
            cross_validation_score=0.0
        )

# Example usage
def test_industry_scoring():
    """Test the industry scoring system"""
    scorer = IndustryStandardScorer()
    
    # Mock signals for testing
    from datetime import datetime
    
    class MockSignal:
        def __init__(self, source, content, engagement, sentiment, timestamp, keywords):
            self.source = source
            self.content = content
            self.engagement_score = engagement
            self.sentiment_score = sentiment
            self.timestamp = timestamp
            self.keywords = keywords
    
    # Create test signals
    test_signals = [
        MockSignal('reddit', 'AI automation startup growing fast', 150, 0.8, datetime.now() - timedelta(hours=2), ['ai', 'automation']),
        MockSignal('github', 'Machine learning platform for businesses', 89, 0.7, datetime.now() - timedelta(hours=6), ['ai', 'platform']),
        MockSignal('hacker_news', 'New AI tool disrupting customer service', 234, 0.6, datetime.now() - timedelta(hours=12), ['ai', 'tool']),
        MockSignal('stackoverflow', 'How to implement AI chatbot API', 45, 0.5, datetime.now() - timedelta(hours=18), ['ai', 'api']),
        MockSignal('dev_to', 'Building AI-powered SaaS applications', 67, 0.7, datetime.now() - timedelta(hours=24), ['ai', 'saas'])
    ]
    
    metrics = scorer.calculate_industry_score(test_signals, 'ai')
    
    print("üèÜ INDUSTRY STANDARD SCORING RESULTS:")
    print(f"üìä Opportunity Score: {metrics.opportunity_score:.2f}/10")
    print(f"üöÄ Momentum Score: {metrics.momentum_score:.2f}/10")
    print(f"‚ö° Trend Velocity: {metrics.trend_velocity:.2f}/10")
    print(f"üéØ Market Penetration: {metrics.market_penetration:.2f}/10")
    print(f"üí° Innovation Index: {metrics.innovation_index:.2f}/10")
    print(f"üìà Confidence Score: {metrics.confidence_score:.2f}/10")
    print(f"üî¨ Statistical Significance: {metrics.statistical_significance:.2f}")
    print(f"‚ö†Ô∏è Risk Score: {metrics.risk_score:.2f}")
    print(f"‚úÖ Signal Quality: {metrics.signal_quality_score:.2f}/10")
    print(f"üîó Source Diversity: {metrics.source_diversity_score:.2f}/10")
    print(f"‚è∞ Temporal Consistency: {metrics.temporal_consistency_score:.2f}/10")
    print(f"üîÑ Cross Validation: {metrics.cross_validation_score:.2f}/10")

if __name__ == "__main__":
    test_industry_scoring() 