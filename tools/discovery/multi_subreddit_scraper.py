#!/usr/bin/env python3
"""
Multi-Subreddit Pain Point Scraper
Scrapes 10 subreddits for real business pain points and SaaS opportunities
"""

import sys
import os
sys.path.append('src')

import asyncio
import json
from datetime import datetime
from collections import defaultdict

from api.domains.discovery.services.discovery_service import DiscoveryService

async def scrape_multiple_subreddits():
    """Scrape 10 subreddits for comprehensive pain point analysis"""
    
    print("ğŸš€ MULTI-SUBREDDIT PAIN POINT SCRAPER")
    print("=" * 60)
    
    # Target subreddits for comprehensive analysis
    target_subreddits = [
        'entrepreneur',      # Main entrepreneurship discussions
        'startups',         # Startup-specific problems
        'SaaS',            # SaaS business challenges
        'smallbusiness',   # Small business pain points
        'freelance',       # Freelancer problems
        'indiehackers',    # Indie hacker challenges
        'business',        # General business issues
        'marketing',       # Marketing pain points
        'webdev',          # Developer problems
        'productivity'     # Productivity challenges
    ]
    
    # Initialize discovery service
    service = DiscoveryService()
    
    # Storage for all results
    all_pain_points = []
    all_opportunities = []
    all_concepts = []
    subreddit_stats = {}
    
    print(f"ğŸ¯ Target Subreddits: {len(target_subreddits)}")
    print(f"ğŸ“Š Posts per subreddit: 8")
    print(f"ğŸ” Total posts to analyze: {len(target_subreddits) * 8}")
    print("\n" + "="*60)
    
    # Scrape each subreddit
    for i, subreddit in enumerate(target_subreddits, 1):
        print(f"\nğŸ” [{i}/{len(target_subreddits)}] SCRAPING r/{subreddit}")
        print("-" * 40)
        
        try:
            # Run discovery on this subreddit
            result = await service.discover_pain_points(
                subreddit=subreddit,
                limit=8  # 8 posts per subreddit = 80 total posts
            )
            
            # Extract results
            posts_analyzed = result.get('posts_analyzed', 0)
            pain_points = result.get('pain_points', [])
            opportunities = result.get('ranked_opportunities', [])
            concepts = result.get('concepts', [])
            
            # Store stats
            subreddit_stats[subreddit] = {
                'posts_analyzed': posts_analyzed,
                'pain_points_found': len(pain_points),
                'opportunities_generated': len(opportunities),
                'concepts_generated': len(concepts)
            }
            
            # Add subreddit tag to all items
            for pain_point in pain_points:
                pain_point['source_subreddit'] = subreddit
                all_pain_points.append(pain_point)
            
            for opportunity in opportunities:
                opportunity['source_subreddit'] = subreddit
                all_opportunities.append(opportunity)
            
            for concept in concepts:
                concept['source_subreddit'] = subreddit
                all_concepts.append(concept)
            
            # Show immediate results
            print(f"   ğŸ“Š Posts: {posts_analyzed}")
            print(f"   âš ï¸ Pain Points: {len(pain_points)}")
            print(f"   ğŸŒŸ Opportunities: {len(opportunities)}")
            print(f"   ğŸ’¡ Concepts: {len(concepts)}")
            
            # Show top pain point from this subreddit
            if pain_points:
                top_pain = pain_points[0]
                print(f"   ğŸ† Top Pain: {top_pain.get('title', 'Unknown')[:60]}...")
            
            # Small delay to be respectful to Reddit
            await asyncio.sleep(2)
            
        except Exception as e:
            print(f"   âŒ Error scraping r/{subreddit}: {str(e)}")
            subreddit_stats[subreddit] = {
                'posts_analyzed': 0,
                'pain_points_found': 0,
                'opportunities_generated': 0,
                'concepts_generated': 0,
                'error': str(e)
            }
    
    # Comprehensive analysis
    print(f"\nğŸ‰ MULTI-SUBREDDIT SCRAPING COMPLETE!")
    print("=" * 60)
    
    # Overall stats
    total_posts = sum(stats['posts_analyzed'] for stats in subreddit_stats.values())
    total_pain_points = len(all_pain_points)
    total_opportunities = len(all_opportunities)
    total_concepts = len(all_concepts)
    
    print(f"ğŸ“Š OVERALL STATISTICS:")
    print(f"   ğŸ“ Total Posts Analyzed: {total_posts}")
    print(f"   âš ï¸ Total Pain Points Found: {total_pain_points}")
    print(f"   ğŸŒŸ Total Opportunities Generated: {total_opportunities}")
    print(f"   ğŸ’¡ Total SaaS Concepts: {total_concepts}")
    print(f"   ğŸ“ˆ Pain Point Discovery Rate: {(total_pain_points/total_posts*100):.1f}%")
    
    # Subreddit performance ranking
    print(f"\nğŸ† SUBREDDIT PERFORMANCE RANKING:")
    sorted_subreddits = sorted(
        subreddit_stats.items(), 
        key=lambda x: x[1]['pain_points_found'], 
        reverse=True
    )
    
    for i, (subreddit, stats) in enumerate(sorted_subreddits[:10], 1):
        if 'error' not in stats:
            print(f"   #{i} r/{subreddit}: {stats['pain_points_found']} pain points from {stats['posts_analyzed']} posts")
        else:
            print(f"   #{i} r/{subreddit}: ERROR - {stats['error']}")
    
    # Top pain points across all subreddits
    print(f"\nâš ï¸ TOP 10 PAIN POINTS ACROSS ALL SUBREDDITS:")
    sorted_pain_points = sorted(all_pain_points, key=lambda x: x.get('score', 0), reverse=True)
    
    for i, pain_point in enumerate(sorted_pain_points[:10], 1):
        title = pain_point.get('title', 'Unknown')
        score = pain_point.get('score', 0)
        subreddit = pain_point.get('source_subreddit', 'unknown')
        industry = pain_point.get('business_context', {}).get('industry', 'Unknown')
        
        print(f"\n   #{i} ğŸ“ˆ {title[:70]}...")
        print(f"       ğŸ“Š Score: {score}/10 | ğŸ­ Industry: {industry} | ğŸ“ r/{subreddit}")
    
    # Top opportunities
    print(f"\nğŸŒŸ TOP 10 SAAS OPPORTUNITIES:")
    sorted_opportunities = sorted(all_opportunities, key=lambda x: x.get('score', 0), reverse=True)
    
    for i, opp in enumerate(sorted_opportunities[:10], 1):
        title = opp.get('title', 'Unknown')
        score = opp.get('score', 0)
        domain = opp.get('domain', 'Unknown')
        subreddit = opp.get('source_subreddit', 'unknown')
        potential = opp.get('business_potential', 'Unknown')
        
        print(f"\n   #{i} ğŸ’¡ {title[:70]}...")
        print(f"       ğŸ“Š Score: {score}/10 | ğŸ­ Domain: {domain} | ğŸ’° Potential: {potential} | ğŸ“ r/{subreddit}")
    
    # Industry analysis
    print(f"\nğŸ­ INDUSTRY BREAKDOWN:")
    industry_counts = defaultdict(int)
    for pain_point in all_pain_points:
        industry = pain_point.get('business_context', {}).get('industry', 'Unknown')
        industry_counts[industry] += 1
    
    sorted_industries = sorted(industry_counts.items(), key=lambda x: x[1], reverse=True)
    for industry, count in sorted_industries[:8]:
        print(f"   ğŸ¯ {industry.title()}: {count} pain points")
    
    # Save comprehensive results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"multi_subreddit_results_{timestamp}.json"
    
    comprehensive_results = {
        'timestamp': timestamp,
        'subreddits_analyzed': target_subreddits,
        'overall_stats': {
            'total_posts': total_posts,
            'total_pain_points': total_pain_points,
            'total_opportunities': total_opportunities,
            'total_concepts': total_concepts,
            'discovery_rate': total_pain_points/total_posts if total_posts > 0 else 0
        },
        'subreddit_stats': subreddit_stats,
        'all_pain_points': all_pain_points,
        'all_opportunities': all_opportunities,
        'all_concepts': all_concepts,
        'industry_breakdown': dict(industry_counts)
    }
    
    with open(results_file, 'w') as f:
        json.dump(comprehensive_results, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ RESULTS SAVED TO: {results_file}")
    print(f"ğŸ¯ Multi-subreddit pain point discovery complete!")
    
    return comprehensive_results

if __name__ == "__main__":
    results = asyncio.run(scrape_multiple_subreddits())
    
    print(f"\nğŸš€ SCRAPING MISSION ACCOMPLISHED!")
    print(f"   ğŸ“Š {results['overall_stats']['total_posts']} posts analyzed")
    print(f"   âš ï¸ {results['overall_stats']['total_pain_points']} pain points discovered")
    print(f"   ğŸŒŸ {results['overall_stats']['total_opportunities']} opportunities generated")
    print(f"   ğŸ“ˆ {results['overall_stats']['discovery_rate']:.1%} success rate") 